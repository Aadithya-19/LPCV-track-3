"""
#Required installations

!pip install -U "transformers>=4.35.0" accelerate safetensors
# If Transformers is very new model-type only in main branch, install from source:
!pip install -U git+https://github.com/huggingface/transformers.git

"""

from huggingface_hub import InferenceClient
import time
import os
import re


#Example ground truth
ground_truth = """
This image is fake because multiple visual cues suggest an AI-generated scene rather than a natural photograph. At a high level, the forest geometry and foliage patterns show unnatural symmetry and repetition, and the lighting on trees, leaves, and water is inconsistent with the strong backlight implied by the bright, misty background. These semantic and physical issues, together with hyper-uniform textures, oversaturated colors, and stylized depth-of-field, strongly support the conclusion that the image is AI-generated"""

#Example output
llm_output = """
[BEGIN]: {
  "verdict": "AI",
  "confidence": 9,
  "explanation 1": "The image appears to be a digital creation, likely generated by AI due to the uniform lighting and consistent shadows.",
  "explanation 2": "The edges and boundaries are sharp, indicating a digital source rather than a real photograph.",
  "explanation 3": "The texture and resolution are high, suggesting a detailed digital creation rather than a low-resolution photograph.",
  "explanation 4": "The perspective and spatial relationships are consistent, indicating a well-composed digital image rather than a random photograph.",
  "explanation 5": "The human and biological structure integrity is not present, as the image is a digital creation.",
  "explanation 6": "The material and object details are not present, as the image is a digital creation.",
  "explanation 7": "The lighting and shadows are consistent, indicating a digital creation rather than a low-resolution photograph."
}[END]"""

token = "<ADD API KEY>"
client = InferenceClient(api_key=token)

# Simple chat completion example
start = time.time()
response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[{"role": "user", "content": "Say hello"}]
)

print(response.choices[0].message.content)

def evaluate_similarity(user_output: str, ground_truth: str) -> float:
    prompt = f"""
You are an evaluation model. Compare the USER OUTPUT with the GROUND TRUTH.
Evaluate **meaning**, **correctness**, and **reasoning similarity**.

Steps:
1. Summarize the GROUND TRUTH reasoning.
2. Summarize the USER OUTPUT reasoning.
3. Identify whether the final conclusion is correct.
4. Score reasoning similarity:
   - 1.0 = identical reasoning
   - 0.5 = different reasoning but correct conclusion
   - 0.0 = incorrect or unrelated reasoning
5. Score content similarity (semantic meaning):
   - 1.0 = identical meaning
   - 0.0 = completely different
6. Produce a final score (0â€“1) balancing **reasoning + meaning**.

Output ONLY the final numeric score.

GROUND TRUTH:
{ground_truth}

USER OUTPUT:
{user_output}
"""

    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[{"role": "user", "content": prompt}]
    )

    elapsed = time.time() - start
    text = response.choices[0].message.content.strip()

    # Try to convert to float
    try:
        return float(text), elapsed
    except:
        nums = re.findall(r"0?\.\d+|1|0", text)
        return (float(nums[0]) if nums else 0.0), elapsed

score = evaluate_similarity(llm_output, 
)
print("Similarity:", score)
